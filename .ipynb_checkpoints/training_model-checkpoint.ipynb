{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eb_P-vOyV2qz"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m boxcox\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, accuracy_score, mean_squared_error, precision_score, recall_score\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, precision_score, recall_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
    "import kerastuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/b8/75/ce4d8eeb1fb100726634358411bc4a8b12f889f6ce560b0973c0a5dbac39/tensorflow-2.16.1-cp311-cp311-macosx_10_15_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/bf/45/c961e3cb6ddad76b325c163d730562bb6deb1ace5acbed0306f5fbefb90e/flatbuffers-24.3.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.10.0 from https://files.pythonhosted.org/packages/c3/99/570fedd40048daeb04d4738ed4f1d0e77259fb631387f7f188aac3d85c31/h5py-3.10.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading h5py-3.10.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/c9/ea/fe2a69cc6cfebf7c7ee8a6357566fc1cbb91632bde5869b669a396accb5f/libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes~=0.3.1 from https://files.pythonhosted.org/packages/6e/a4/6aabb78f1569550fd77c74d2c1d008b502c8ce72776bd88b14ea6c182c9e/ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/f3/bf/26deba06a4c910a85f78245cac7698f67cedd7efe00d04f6b3e1b3506a59/protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/c1/0a/a8c0f403b2189f5d3e490778ead51924b56fa30a35f6e444b3702e28c8c8/grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.17,>=2.16 from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras>=3.0.0 from https://files.pythonhosted.org/packages/b0/b2/104733bb67fde86f3d10010f0b5c93cfa1d5bf552f904584cf9e5b3ba719/keras-3.0.5-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.0.5-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/ad/e3/1009781ce3c0d92634fa2fb3dc4bb0237fe7aaf70f2ab53160f3e82e7d63/tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/cd/43/b971880e2eb45c0bee2093710ae8044764a89afe9620df34a231c6f0ecd2/namex-0.0.7-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting dm-tree (from keras>=3.0.0->tensorflow)\n",
      "  Obtaining dependency information for dm-tree from https://files.pythonhosted.org/packages/b1/65/4f10a68dde5fa0c91043c9c899e9bc79b1657ba932d39a5f8525c0058e68/dm_tree-0.1.8-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-macosx_10_9_x86_64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdown>=2.6.8 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/b7/85/dabeaf902892922777492e1d253bb7e1264cadce3cea932f7ff599e53fea/tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/kennethhie/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-macosx_10_15_x86_64.whl (259.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.6/259.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.10.0-cp311-cp311-macosx_10_9_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_10_14_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dm_tree-0.1.8-cp311-cp311-macosx_10_9_x86_64.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, dm-tree, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, rich, keras, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzHqhwENV2q4"
   },
   "source": [
    "# Data Cleaning (KaggleMovies.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df is going to have all of our data about each movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "jkdvH2YNV2q7",
    "outputId": "3144f991-3069-4222-f27e-03ac0079734d"
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('Datasets/KaggleMovies.csv')\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "pd.set_option('display.precision', 2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmOp3uAsV2q7"
   },
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "column_mapping = {\n",
    "    'name' : 'Name',\n",
    "    'rating' : 'Rating',\n",
    "    'genre' : 'Genre',\n",
    "    'year' : 'Year',\n",
    "    'released' : 'Released',\n",
    "    'score' : 'Score',\n",
    "    'votes' : 'Votes',\n",
    "    'director' : 'Director',\n",
    "    'writer' : 'Writer',\n",
    "    'star' : 'Star',\n",
    "    'country' : 'Country',\n",
    "    'budget' : 'Budget',\n",
    "    'gross' : 'Gross Revenue',\n",
    "    'company' : 'Company',\n",
    "    'runtime' : 'Runtime'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3SJUA4qtV2q8",
    "outputId": "9daa334d-b255-41c7-96c3-18ab87ea0ea1"
   },
   "outputs": [],
   "source": [
    "# Check for empty spaces\n",
    "print(f\"Shape: {df.shape}\\n\")\n",
    "isna = df.isna().sum(axis=0)\n",
    "isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cd74dCFV2q8",
    "outputId": "6b378832-c5c3-4410-9eca-c17d218dc544"
   },
   "outputs": [],
   "source": [
    "# Remove rows with null values\n",
    "df = df.dropna()\n",
    "print(f\"Shape: {df.shape}\\n\")\n",
    "print(df.isna().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "Bncy_-emV2q9",
    "outputId": "e27c879c-1cd5-4146-c0a0-99166af43fbe"
   },
   "outputs": [],
   "source": [
    "# Convert certain columns to appropiate data types\n",
    "df.loc[:,['Votes', 'Budget', 'Runtime', 'Gross Revenue']] = df[['Votes', 'Budget', 'Runtime', 'Gross Revenue']].astype('int')\n",
    "df.loc[:,'Score'] = df['Score'].apply(lambda x: float(\"{:.2f}\".format(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYPblPO09Hcr",
    "outputId": "52aaee3c-ccf8-4b0f-99b2-cc63cfa3753b"
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38LFY_LOgZL_",
    "outputId": "67a20ef0-863d-4ed0-d67b-e75087afcabf"
   },
   "outputs": [],
   "source": [
    "# Check number of unique movie names\n",
    "len(df['Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "Fgm1UuIbgukg",
    "outputId": "bde02f6c-8de4-4b18-eeee-f3e1ccec2b42"
   },
   "outputs": [],
   "source": [
    "#Drop duplicate movie names\n",
    "df = df.drop_duplicates(subset=['Name'], keep='first')\n",
    "print(f\"Shape: {df.shape}\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df now contains movies that are unique and have no empty values on any of its attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5dFi6Qa1dlK"
   },
   "source": [
    "### Univariate Analysis (Numerical Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "gAbQ72UBkOoE",
    "outputId": "4beaec7f-c7c6-41a0-d3e0-dc1c76a33733"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "KFhW43DtrxOO",
    "outputId": "d36cc866-dad5-4e4e-d641-7884cffc91f6"
   },
   "outputs": [],
   "source": [
    "#Univariate Analysis of Year\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Year'], kde=True, stat='density')\n",
    "plt.title('Distribution of Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Year'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Year')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "tgA1M_wlhPdC",
    "outputId": "40718f0e-732f-4e28-cbc5-231f83294286"
   },
   "outputs": [],
   "source": [
    "#Univariate Analysis of Votes\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Votes'], kde=True, stat='density')\n",
    "plt.title('Distribution of Votes')\n",
    "plt.xlabel('Votes')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Votes'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Votes')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Votes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "vHaH3whytMKo",
    "outputId": "d1e853ff-dcfc-4abe-dc40-56d27b99d316"
   },
   "outputs": [],
   "source": [
    "#Normalize the right skewed data\n",
    "df['Votes'], lambda_votes = boxcox(df['Votes'])\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Votes'], kde=True, stat='density')\n",
    "plt.title('Distribution of Votes')\n",
    "plt.xlabel('Votes')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Votes'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Votes')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Votes' after normalizing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "8F1rG_g_u4a3",
    "outputId": "f6110ecd-2f89-4016-ecfc-7bfa746163ba"
   },
   "outputs": [],
   "source": [
    "#Univariate Analysis of Gross Revenue\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Gross Revenue'], kde=True, stat='density')\n",
    "plt.title('Distribution of Gross Revenue')\n",
    "plt.xlabel('Gross Revenue')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Gross Revenue'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Gross Revenue')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Gross Revenue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "3WfcjKbR0hJB",
    "outputId": "399e5067-060e-4208-eda0-8e1ed5aa3683"
   },
   "outputs": [],
   "source": [
    "#Normalize the right skewed data\n",
    "df['Gross Revenue'], lambda_gross = boxcox(df['Gross Revenue'])\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Gross Revenue'], kde=True, stat='density')\n",
    "plt.title('Distribution of Gross Revenue')\n",
    "plt.xlabel('Gross Revenue')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Gross Revenue'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Gross Revenue')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Gross Revenue' after normalizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "91PDv-6K2RZH",
    "outputId": "6be92c90-bee1-497e-d75f-fe867c73eb16"
   },
   "outputs": [],
   "source": [
    "#Univariate Analysis of Budget\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Budget'], kde=True, stat='density')\n",
    "plt.title('Distribution of Budget')\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Budget'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Budget')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Budget'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "9gK_Yub-77Xa",
    "outputId": "8e1baa18-3bbd-425a-8849-ff64ee636878"
   },
   "outputs": [],
   "source": [
    "#Univariate Analysis of Runtime\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Runtime'], kde=True, stat='density')\n",
    "plt.title('Distribution of Runtime')\n",
    "plt.xlabel('Runtime')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Runtime'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Runtime')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Runtime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "hpRTfGkz03h1",
    "outputId": "d29c9570-2774-4c98-8341-fb226e06a02f"
   },
   "outputs": [],
   "source": [
    "df['Budget'] = df['Budget'].apply(lambda x: 250000000 if x > 250000000 else x)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "#histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Budget'], kde=True, stat='density')\n",
    "plt.title('Distribution of Budget')\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(df['Budget'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Budget')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our graphs representing the univariate analysis of 'Budget' after we cap the values above 250,000,000 to 250,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "647IEk6c10zf"
   },
   "source": [
    "### Univariate Analysis (Categorical Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEZgB2BG1OzA",
    "outputId": "da74795b-3fe6-44c4-e940-d5556c46be3b"
   },
   "outputs": [],
   "source": [
    "print(\"Unique Value Count of Categorical Variables\")\n",
    "print(\"Rating:\", len(df['Rating'].unique()))\n",
    "print(\"Genre:\",len(df['Genre'].unique()))\n",
    "print(\"Director:\",len(df['Director'].unique()))\n",
    "print(\"Writer:\",len(df['Writer'].unique()))\n",
    "print(\"Star:\",len(df['Star'].unique()))\n",
    "print(\"Country:\",len(df['Country'].unique()))\n",
    "print(\"Company:\",len(df['Company'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "Qw6i78Sq1793",
    "outputId": "ced0f693-f99e-4797-832a-8c9fb1838bdd"
   },
   "outputs": [],
   "source": [
    "score_by_rating = df.groupby(\"Rating\")[\"Score\"].mean().sort_values(ascending=True)\n",
    "\n",
    "plt.barh(score_by_rating.index, score_by_rating.values)\n",
    "plt.xlabel('Mean Score')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Mean Score by Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This horizontal bar chart displays the average scores for different content ratings of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "iF_Hc92t17qo",
    "outputId": "9a782939-e4c7-492b-cd68-d924b585b743"
   },
   "outputs": [],
   "source": [
    "score_by_genre = df.groupby(\"Genre\")[\"Score\"].mean().sort_values(ascending=True)[len(df['Genre'].unique()) - 10 :]\n",
    "\n",
    "plt.barh(score_by_genre.index, score_by_genre.values)\n",
    "plt.xlabel('Mean Score')\n",
    "plt.ylabel('Genre')\n",
    "plt.title('Mean Score by Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This horizontal bar chart displays the average scores for different movie genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "rgtejCPrwUHQ",
    "outputId": "e6652c57-f349-4cf1-a1f7-ad78a88880be"
   },
   "outputs": [],
   "source": [
    "count_by_genre = df.groupby(\"Genre\")[\"Score\"].count().sort_values(ascending=True)[len(df['Genre'].unique()) - 10 :]\n",
    "\n",
    "plt.barh(count_by_genre.index, count_by_genre.values)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Genre')\n",
    "plt.title('Count by Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart illustrates the distribution across various genres of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "4TdgG6wg5ITM",
    "outputId": "8a3b0427-0dfe-45ed-df29-c9e0a90eacf4"
   },
   "outputs": [],
   "source": [
    "score_by_country = df.groupby(\"Country\")[\"Score\"].mean().sort_values(ascending=True)[len(df['Country'].unique()) - 10 :]\n",
    "\n",
    "plt.barh(score_by_country.index, score_by_country.values)\n",
    "plt.xlabel('Mean Score')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Mean Score by Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This horizontal bar chart displays the average scores of movies released by various countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "OtkH2CSq5x2j",
    "outputId": "4fba28d8-2280-4997-ba69-90ba1ac29267"
   },
   "outputs": [],
   "source": [
    "count_by_country = df.groupby(\"Country\")[\"Score\"].count().sort_values(ascending=True)[len(df['Country'].unique()) - 10 :]\n",
    "\n",
    "plt.barh(count_by_country.index, count_by_country.values)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Count by Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This horizontal bar chart displays the number of movies released by various countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "jkdcKV2UvcZ4",
    "outputId": "76a7f3d8-042c-4b2b-d7f1-712d918c3922"
   },
   "outputs": [],
   "source": [
    "score_by_company = df.groupby(\"Company\")[\"Score\"].mean().sort_values(ascending=True)[len(df['Company'].unique()) - 10 :]\n",
    "\n",
    "plt.barh(score_by_company.index, score_by_company.values)\n",
    "plt.xlabel('Mean Score')\n",
    "plt.ylabel('Company')\n",
    "plt.title('Mean Score by Company')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This horizontal bar chart displays the average score released by various companies ordered from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "9dSyplM5vxkX",
    "outputId": "2cd1c96f-1e78-4118-f52e-1cee9c797a9b"
   },
   "outputs": [],
   "source": [
    "score_by_company = df.groupby(\"Company\")[\"Score\"].mean().sort_values(ascending=False)[len(df['Company'].unique()) - 10 :]\n",
    "\n",
    "plt.barh(score_by_company.index, score_by_company.values)\n",
    "plt.xlabel('Mean Score')\n",
    "plt.ylabel('Company')\n",
    "plt.title('Mean Score by Company')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This horizontal bar chart displays the average score released by various companies ordered in lowest to highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "Ejo4mSdmv1vS",
    "outputId": "ea064a19-f6bd-44de-ebd8-d9dc1a1fc7d6"
   },
   "outputs": [],
   "source": [
    "score_by_company = df.groupby(\"Company\")[\"Score\"].count().sort_values(ascending=True)[len(df['Company'].unique()) - 10 :]\n",
    "\n",
    "plt.barh(score_by_company.index, score_by_company.values)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Company')\n",
    "plt.title('Count by Company')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This horizontal bar chart displays the total count released by various companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcbLv5ZjuygT"
   },
   "source": [
    "### Data Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "lHrWpxIJ5atz",
    "outputId": "9e88230e-c5ae-44d7-ca94-a7805b467040"
   },
   "outputs": [],
   "source": [
    "# Instantiating MinMaxScaler and StandardScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "# Scaling the columns\n",
    "df[['Budget']] = scaler_minmax.fit_transform(df[['Budget']])\n",
    "df[['Votes', 'Gross Revenue', 'Runtime']] = scaler_std.fit_transform(df[['Votes', 'Gross Revenue', 'Runtime']])\n",
    "\n",
    "# Performing one-hot encoding on 'Rating', 'Genre', and 'Country' columns\n",
    "oneHot = pd.get_dummies(df[['Rating', 'Genre', 'Country']], columns=['Rating', 'Genre', 'Country'])\n",
    "df = pd.concat([df, oneHot], axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7iplva8053x"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "HhWUXeHd0-Al",
    "outputId": "26635518-18c3-4b7d-f897-892da9e2c3ce"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x=\"Year\", y=\"Score\")\n",
    "\n",
    "plt.title('Score Throughout the Years')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our graph representing the Bivariate analysis of 'Score' throughout the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "YqSPWnMr7y-u",
    "outputId": "c7d109a3-afb3-48ab-9549-49ae9dad71a8"
   },
   "outputs": [],
   "source": [
    "correlation_matrix = df[[\"Year\", 'Score', \"Votes\", \"Budget\", \"Gross Revenue\", \"Runtime\"]].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model 1: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our correlation matrix of attributes that have numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "0GU3wQ3G0cbT",
    "outputId": "2fca679f-e1ec-42ff-9e20-bdcfd860308c"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "df_BGR = df[['Budget', 'Gross Revenue']]\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(df_BGR.Budget, df_BGR.drop(['Budget'], axis=1), test_size=0.2, random_state=21)\n",
    "\n",
    "train = pd.concat([X1_train, y1_train], axis=1)\n",
    "train = train.sort_values(by=['Budget'])\n",
    "budget = train[['Budget']]\n",
    "grossRevenue = train[['Gross Revenue']]\n",
    "\n",
    "# 2 Degrees\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "polyX = poly2.fit_transform(budget)\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "linear.fit(polyX, grossRevenue)\n",
    "yhat = linear.predict(polyX)\n",
    "\n",
    "sns.scatterplot(data=df, x=\"Budget\", y=\"Gross Revenue\")\n",
    "plt.plot(budget, yhat, color=\"green\")\n",
    "plt.title(\"Polynomial Regression: Budget vs. Gross Revenue (2 Degrees)\")\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel('Gross Revenue')\n",
    "plt.show()\n",
    "\n",
    "# 3 Degrees\n",
    "poly3 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "polyX = poly3.fit_transform(budget)\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "linear.fit(polyX, grossRevenue)\n",
    "yhat = linear.predict(polyX)\n",
    "\n",
    "sns.scatterplot(data=df, x=\"Budget\", y=\"Gross Revenue\")\n",
    "plt.plot(budget, yhat, color=\"orange\")\n",
    "plt.title(\"Polynomial Regression: Budget vs. Gross Revenue (3 Degrees)\")\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel('Gross Revenue')\n",
    "plt.show()\n",
    "\n",
    "# 3 Degrees\n",
    "poly4 = PolynomialFeatures(degree=4, include_bias=False)\n",
    "polyX = poly4.fit_transform(budget)\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "linear.fit(polyX, grossRevenue)\n",
    "yhat = linear.predict(polyX)\n",
    "\n",
    "sns.scatterplot(data=df, x=\"Budget\", y=\"Gross Revenue\")\n",
    "plt.plot(budget, yhat, color=\"red\")\n",
    "plt.title(\"Polynomial Regression: Budget vs. Gross Revenue (4 Degrees)\")\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel('Gross Revenue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our graph representing the Bivariate analysis of 'Budget' vs 'Gross Reveue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "tyD7ueX27Ud6",
    "outputId": "b03c2527-492b-4254-88ae-f2423f326fc3"
   },
   "outputs": [],
   "source": [
    "df_GRV = df[['Gross Revenue', 'Votes']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_GRV.drop(['Votes'], axis=1), df_GRV.Votes, test_size=0.2, random_state=21)\n",
    "\n",
    "model = LinearRegression()\n",
    "# Fit model to the data\n",
    "model.fit(X_train, y_train)\n",
    "# Predict Y values using the fitted model\n",
    "Y_pred = model.predict(X_train)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X_train, Y_pred, color='red', label='Scikit Linear Regression')\n",
    "sns.scatterplot(data=df, x=\"Gross Revenue\", y=\"Votes\")\n",
    "plt.title('Gross Revenue vs Votes')\n",
    "plt.xlabel('Gross Revenue')\n",
    "plt.ylabel('Votes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our graph representing the Bivariate analysis of 'Gross Revenue' vs 'Votes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "x63y1eaW9YzG",
    "outputId": "674b1009-cc63-452e-8dea-beaaae4b0481"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "df_VS = df[['Votes', 'Score']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_VS.drop(['Score'], axis=1), df_VS.Score, test_size=0.2, random_state=21)\n",
    "\n",
    "model = LinearRegression()\n",
    "# Fit model to the data\n",
    "model.fit(X_train, y_train)\n",
    "# Predict Y values using the fitted model\n",
    "Y_pred = model.predict(X_train)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(X_train, Y_pred, color='red', label='Scikit Linear Regression')\n",
    "sns.scatterplot(data=df, x=\"Votes\", y=\"Score\")\n",
    "plt.title('Votes vs Score')\n",
    "plt.xlabel('Votes')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our graph representing the Bivariate analysis of 'Vote' vs 'Score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our First Model Compare Training vs Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate and plot polynomial regression\n",
    "def evaluate_poly_regression(degree, X_train, y_train, X_test, y_test, color):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    polyX_train = poly.fit_transform(X_train)\n",
    "    polyX_test = poly.transform(X_test)\n",
    "    \n",
    "    linear = LinearRegression()\n",
    "    linear.fit(polyX_train, y_train)\n",
    "    \n",
    "    # Predict on training and testing data\n",
    "    yhat_train = linear.predict(polyX_train)\n",
    "    yhat_test = linear.predict(polyX_test)\n",
    "    \n",
    "    # Calculate training and testing errors\n",
    "    mse_train = mean_squared_error(y_train, yhat_train)\n",
    "    mse_test = mean_squared_error(y_test, yhat_test)\n",
    "    \n",
    "    # Plotting for training data\n",
    "    # Convert DataFrame/Series to numpy array and then flatten\n",
    "    X_train_np = X_train.to_numpy().flatten() if hasattr(X_train, 'to_numpy') else X_train.flatten()\n",
    "    y_train_np = y_train.to_numpy().flatten() if hasattr(y_train, 'to_numpy') else y_train.flatten()\n",
    "    sns.scatterplot(x=X_train_np, y=y_train_np, label='Training Data')\n",
    "    \n",
    "    # Sort the values for better plotting\n",
    "    sorted_train_indices = np.argsort(X_train_np)\n",
    "    plt.plot(X_train_np[sorted_train_indices], yhat_train[sorted_train_indices], color=color, label=f'Train Fit (Degree {degree})')\n",
    "    \n",
    "    # Plotting for test data\n",
    "    X_test_np = X_test.to_numpy().flatten() if hasattr(X_test, 'to_numpy') else X_test.flatten()\n",
    "    y_test_np = y_test.to_numpy().flatten() if hasattr(y_test, 'to_numpy') else y_test.flatten()\n",
    "    sns.scatterplot(x=X_test_np, y=y_test_np, label='Test Data', color='lightgrey')\n",
    "    \n",
    "    # Sort the values for better plotting\n",
    "    sorted_test_indices = np.argsort(X_test_np)\n",
    "    plt.plot(X_test_np[sorted_test_indices], yhat_test[sorted_test_indices], color=color, linestyle='--', label=f'Test Fit (Degree {degree})')\n",
    "    plt.title(f\"Polynomial Regression: Budget vs. Gross Revenue ({degree} Degrees)\")\n",
    "    plt.xlabel('Budget')\n",
    "    plt.ylabel('Gross Revenue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print errors\n",
    "    print(f\"Degree {degree} polynomial regression training MSE: {mse_train}\")\n",
    "    print(f\"Degree {degree} polynomial regression testing MSE: {mse_test}\")\n",
    "\n",
    "\n",
    "# Evaluate and plot for 2 degrees\n",
    "evaluate_poly_regression(2, budget, grossRevenue, X1_test.values.reshape(-1, 1), y1_test, \"green\")\n",
    "\n",
    "# Evaluate and plot for 3 degrees\n",
    "evaluate_poly_regression(3, budget, grossRevenue, X1_test.values.reshape(-1, 1), y1_test, \"orange\")\n",
    "\n",
    "# Evaluate and plot for 4 degrees\n",
    "evaluate_poly_regression(4, budget, grossRevenue, X1_test.values.reshape(-1, 1), y1_test, \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three degrees 2, 3, and 4 are showing a good fit as seen by the testing MSE being lower than or very close to the training MSE. There is no evidence of overfitting, where we would expect the testing MSE to be significantly higher than the training MSE due to the model capturing noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fit In The Fitting Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our MSE values, as the polynomial degree increases from 2 to 4, both training and testing MSEs decrease. This indicates that our model is capturing more of the data's underlying pattern with increased complexity, improving its performance. Our model with polynomial degree 4 have the lowest MSEs suggests an optimal balance between bias and variance, making it the best fit among the ones that we tested. There's no sign of overfitting as both training and testing error decreases together. In conclusion, our polynomial regression model with degree 4 provides the best balance of complexity and performance based on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next 2 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of the insights derived from the performance of our current neural network model, we are considering the exploration of two additional machine learning models to potentially enhance our predictive capabilities and address the identified limitations:\n",
    "\n",
    "- **Neural Network**: Neural networks are indeed well-suited to handle the complexity inherent in modeling the relationship between a movie's budget and its gross revenue, thanks to their ability to model nonlinear relationships and interactions between features without the need for manual feature engineering. Unlike polynomial regression, which requires choosing the degree of polynomials a priori and risks overfitting with higher degrees, neural networks can learn complex patterns through their hidden layers and neurons. They do this by adjusting weights and biases through backpropagation based on the error rate, allowing them to capture both high-level and subtle nuances in data. Moreover, neural networks can automatically discover the interaction between variables, making them a powerful tool for capturing the multifaceted dynamics of movie revenues. The flexibility and adaptability of neural networks, combined with techniques to prevent overfitting such as dropout and regularization, make them an attractive option for improving upon traditional regression models in predicting outcomes with complex, non-linear relationships.\n",
    "\n",
    "- **Decision Tree Classifier**: Despite the simplicity of decision trees, they are powerful for classification tasks and provide clear visualization of the decision-making process. A key advantage of using a Decision Tree Classifier is its interpretability; it allows for easy understanding of how decisions are made, which is invaluable for analyzing which features most significantly affect a movie's success. Furthermore, decision trees can handle non-linear data effectively and are less susceptible to outliers than regression models, making them a suitable choice for further exploration.\n",
    "\n",
    "These models were chosen with the intention of addressing specific challenges observed in our initial approach. Polynomial regression will allow us to test the hypothesis that a more nuanced modeling of relationships between variables could yield better predictive performance. On the other hand, the Decision Tree Classifier offers a different approach to classification, with the potential for higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the conclusion of your 1st model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement in MSE as the degree of the polynomial increases suggests that the relationship between budget and gross revenue is complex and potentially non-linear, with higher-degree polynomials capturing this complexity more effectively. The consistent decrease in both training and testing errors indicates that the model is not yet suffering from overfitting at the fourth degree.\n",
    "\n",
    "However, it's important to note that while the improvements in MSE are consistent, they are also marginal, especially when moving from degree 3 to degree 4. This diminishing return suggests that there is a limit to how much more complexity (in terms of polynomial degree) can beneficially be added to the model without overfitting. The observation of diminishing returns as we increase the polynomial degree suggests a critical insight into the nature of modeling complex relationships, such as that between a movie's budget and its gross revenue. It emphasizes the inherent trade-offs in model development, especially between capturing the underlying data patterns (reducing bias) and maintaining a model's ability to generalize well to unseen data (avoiding overfitting).\n",
    "\n",
    "The fact that the model is not yet overfitting at the fourth degree is encouraging, indicating there's still some, albeit limited, scope for complexity increase without sacrificing model performance on new data. However, the marginal gains observed caution us against pursuing higher degrees of polynomial without careful consideration. It suggests that we are approaching, if not already at, the point of optimal complexity where the model is sufficiently complex to capture the relevant patterns in the data but not so complex that it becomes overly specialized to the training set.\n",
    "\n",
    "This situation underscores the importance of exploring alternative strategies for model improvement that do not solely rely on increasing model complexity through higher-degree polynomials. Techniques such as incorporating domain knowledge to engineer more relevant features, employing regularization methods to penalize unnecessary complexity, and exploring other forms of model validation like cross-validation to ensure that improvements are robust and generalizable, become paramount.\n",
    "\n",
    "Moreover, this context also highlights the potential utility of exploring other modeling approaches that might inherently balance complexity and generalizability better. Machine learning models, such as random forests, gradient boosting machines, or neural networks, offer sophisticated mechanisms to model non-linear relationships and interactions without manually specifying the form of the model. These models come with their mechanisms to control overfitting, such as depth limitations in trees or dropout in neural networks, potentially providing a more effective way to capture the complexities of the relationship between movie budgets and gross revenue while maintaining good performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can be done to possibly to improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regularization**: To prevent overfitting, especially when using higher-degree polynomials, consider applying regularization techniques such as Ridge or Lasso regression. These methods can help control the complexity of the model by penalizing large coefficients.\n",
    "\n",
    "- **Feature Engineering**: Besides polynomial features, explore other forms of feature engineering. For instance, interaction terms between budget and other variables might provide additional insights. Also, normalizing or scaling the features might help, especially when moving towards models that use regularization.\n",
    "\n",
    "- **Alternative Models**: Consider exploring non-linear models beyond polynomials, such as decision trees, random forests, or gradient boosting machines, which might capture the data's complexity in different ways.\n",
    "\n",
    "- **Hyperparameter Tuning**: Use grid search or random search to find the optimal combination of hyperparameters, such as the degree of the polynomial and regularization strength. This systematic approach can help in identifying the best model configuration.\n",
    "\n",
    "- **Non-Linear Transformations**: Before applying polynomial features, consider non-linear transformations on the input features, such as logarithmic, square root, or exponential transformations. These transformations can help in linearizing relationships between features and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model 2: Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, let's define the function to classify scores\n",
    "def classify_score(score):\n",
    "    if score <= 3:\n",
    "        return 'very bad'\n",
    "    elif score <= 5:\n",
    "        return 'bad'\n",
    "    elif score <= 8:\n",
    "        return 'mid'\n",
    "    else:\n",
    "        return 'good'\n",
    "\n",
    "# Create a new column 'group_rating' based on the score\n",
    "df['group_rating'] = df['Score'].apply(classify_score)\n",
    "\n",
    "# Selecting the required columns for features and target variable\n",
    "X_movies = df[['Budget', 'Gross Revenue']]  # Features: budget and gross revenue\n",
    "y_movies = df['group_rating']  # Target variable is now 'group_rating'\n",
    "\n",
    "# Apply one-hot encoding to the target variable (rating)\n",
    "encoder_movies = OneHotEncoder()\n",
    "y_movies_encoded = encoder_movies.fit_transform(y_movies.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Apply min-max normalization to the 'budget' and 'gross' features\n",
    "scaler_movies = MinMaxScaler()\n",
    "X_movies_scaled = scaler_movies.fit_transform(X_movies)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Split the data into training and testing sets with a 90:10 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_movies_scaled, y_movies_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "MFXACTIVATION ='relu'\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation= MFXACTIVATION),  # Increased complexity and changed to ReLU\n",
    "    Dense(32, activation=MFXACTIVATION),  # ReLU activation\n",
    "    Dense(16, activation=MFXACTIVATION),  # ReLU activation\n",
    "    Dense(8, activation=MFXACTIVATION),  # ReLU activation\n",
    "    Dense(y_train.shape[1], activation='sigmoid')  # Output layer remains the same\n",
    "])\n",
    "\n",
    "optimizer = SGD(learning_rate=0.001)  # Adjusted learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "acc = accuracy_score(y_test_classes, y_pred_classes)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "\n",
    "# Output the metrics\n",
    "print(\"Confusion Matrix:\", cm)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the model, adapted for the Kaggle movie dataset\n",
    "def build_movie_model():\n",
    "    MFXACTIVATION ='relu'\n",
    "    model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation= MFXACTIVATION),  # Increased complexity and changed to ReLU\n",
    "    Dense(32, activation=MFXACTIVATION),  # ReLU activation\n",
    "    Dense(16, activation=MFXACTIVATION),  # ReLU activation\n",
    "    Dense(8, activation=MFXACTIVATION),  # ReLU activation\n",
    "    Dense(y_train.shape[1], activation='sigmoid')  # Output layer remains the same\n",
    "    ])\n",
    "    model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy', 'mse'])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model with KerasClassifier\n",
    "estimator_movies = KerasClassifier(model=build_movie_model, epochs=100, batch_size=10, verbose=1)  # Adjust verbosity as needed\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "kfold_movies = RepeatedKFold(n_splits=10, n_repeats=1, random_state=42)\n",
    "\n",
    "# Perform cross-validation for accuracy and MSE\n",
    "results_movies = cross_validate(estimator_movies, X_movies_scaled, y_movies_encoded, cv=kfold_movies, scoring=['accuracy', 'neg_mean_squared_error'], n_jobs=1)\n",
    "\n",
    "# Output results\n",
    "print(\"Cross-validation results:\")\n",
    "print(results_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'results_movies' contains the cross-validation results from the previous step\n",
    "cv_accuracy_movies = results_movies['test_accuracy']\n",
    "cv_mse_movies = -1 * results_movies['test_neg_mean_squared_error']  # Multiply by -1 to make MSE positive\n",
    "\n",
    "# Print accuracy and MSE values for each fold\n",
    "for i in range(len(cv_accuracy_movies)):\n",
    "    print(f\"Fold {i+1}: Accuracy = {cv_accuracy_movies[i]}, MSE = {cv_mse_movies[i]}\")\n",
    "\n",
    "# Print the overall average accuracy and MSE for the Kaggle movie dataset\n",
    "print(f\"Overall Average Accuracy: {np.mean(cv_accuracy_movies)}\")\n",
    "print(f\"Overall Average MSE: {np.mean(cv_mse_movies)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_hp, X_test_hp, y_train_hp, y_test_hp = train_test_split(X_movies_scaled, y_movies_encoded, test_size=0.1, random_state=42)\n",
    "def build_hp_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units', min_value=12, max_value=512, step=100),\n",
    "                    activation='relu',\n",
    "                    input_dim=X_train_hp.shape[1]))\n",
    "    model.add(Dense(units=hp.Int('units_layer2', min_value=12, max_value=512, step=100), activation='relu'))\n",
    "    model.add(Dense(units=hp.Int('units_layer3', min_value=12, max_value=512, step=100), activation='relu')) # changed to relu to match previous models\n",
    "    model.add(Dense(units=hp.Int('units_layer4', min_value=12, max_value=512, step=100), activation='relu'))\n",
    "\n",
    "    model.add(Dense(y_train_hp.shape[1], activation='sigmoid'))  # Adapted for multi-class classification\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "    model.compile(optimizer=SGD(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "# Define the tuner\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_hp_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_epochs=10,\n",
    "#     hyperband_iterations=15,\n",
    "#     directory='tuner_results',\n",
    "#     project_name='kaggle_movie_tuning'\n",
    "# )\n",
    "\n",
    "# Define the tuner\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel=build_hp_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    seed=15,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    directory='tuner_results',\n",
    "    project_name='kaggle_movie_tuning'\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(X_train_hp, y_train_hp, epochs=50, validation_split=0.1)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first hidden layer is {best_hps.get('units')}\n",
    "with an optimal learning rate of {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# we used early stopping in earlier versions of our sequential model, so we decide to use it\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy', # or 'val_loss' if you prefer to monitor loss\n",
    "    patience=10,            # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and evaluate it\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(X_train_hp, y_train_hp, epochs=100, validation_data=(X_test_hp, y_test_hp), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our model compare training vs test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "# Calculate metrics\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "acc = accuracy_score(y_test_classes, y_pred_classes)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "\n",
    "# Output the metrics\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Accuracy graph**: The accuracy for both training and validation shoots up sharply and reaches a high level after the first epoch. After the initial jump, the accuracy for both remains approximately constant. It's noteworthy that the validation accuracy is slightly higher than the training accuracy, which is unusual but not necessarily a problem if the variance is small. This could sometimes happen due to the specific samples in the validation set or regularization effects.\n",
    "\n",
    "**Model Loss graph**: The loss for both training and validation decreases sharply after the first epoch and continues to decrease gradually over subsequent epochs. The training and validation losses are very close to each other and converge towards a similar value, which indicates that the model is generalizing well without overfitting or underfitting. There's no sign of divergence, which is good.\n",
    "\n",
    "**Confusion Matrix**: The confusion matrix indicates that our model did not correctly predict any of the samples for the first two classes, but predicted all samples that belonged to the third class. The model correctly identified 487 samples of the third class, which indicates that it has a strong bias towards this class.\n",
    "\n",
    "**MSE**: The MSE is around 0.074, which might seem low, but MSE is not typically the best metric for classification problems, especially when dealing with categorical data.\n",
    "\n",
    "**Accuracy**: The accuracy which is about 91.2% which is a bit misleading in this context, as it seems the model has learned to predict the majority class very well but fails to recognize the other classes.\n",
    "\n",
    "**Precision**: The precision score is about 83.2%. This suggests that when the model predicts a class, it's correct about 83.2% of the time. However, this metric is weighted and might be influenced by the class imbalance.\n",
    "\n",
    "**Recall**: The recall is equal to the accuracy. This means that the model is very good at detecting the positives of the third class. However, for the first two classes, the recall is 0% because the model failed to identify any true positives for these classes.\n",
    "\n",
    "In conclusion, the model is likely suffering from a class imbalance issue, where it predicts the majority class well but fails to predict the minority classes. This is evidenced by the lack of true positives for the two classes in the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fit In The Fitting Graph, How does it compare to your first model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a high accuracy, precision and recall on both the training and validation data, which would suggest a good fit. This suggests that our second model performs well in predicting the \"group ratings\" based on the \"Gross Revenue\" and \"Budget\" of the movie and we can determine that \"Gross Revenue\" and \"Budget\" are good features in order to predict our target variable \"group rating\". \n",
    "\n",
    "The fitting graphs with MSE values for the polynomial regression indicate that the model's performance varies with the degree of the polynomial. As the degree increases, the model fits the training data better but could potentially overfit. While our second model shows high classification accuracy, the fitting graphs from our first model indicate the regression model's capability to predict a continuous outcome based on its input. They serve different purposes and thus, their performances are not directly comparable.\n",
    "\n",
    "In conclusion, for the specific task of classifying movies into 'group ratings', your second model seems to have found a good balance and performs well according to the classification metrics. For the task of predicting 'Gross Revenue' from 'Budget', the regression analysis of our first model indicates a variable fit depending on the polynomial degree, and its performance should be assessed by how well it generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did you perform hyper parameter tuning? K-fold Cross validation? Feature expansion? What were the results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed, Hyper parameter tuning and K-fold Corss validation, but we didn't did any feature expansion as what we did was grouping scores. What we did with Grouping scores mean that We transformed the continuous 'Score' variable into a categorical 'group_rating' variable. This process is a form of feature engineering, where we're creating a new feature based on existing data.\n",
    "\n",
    "1. **K Fold Cross-Validation**\n",
    "- Cross-Validation: Applied Repeated K-Fold cross-validation to assess the model's performance across different subsets of the data, ensuring the model's effectiveness and generalization capability. This approach helps in evaluating the model's stability and reliability across different data splits.\n",
    "- Performance Metrics: The model's accuracy and mean squared error (MSE) were evaluated, providing insights into its classification performance and how close the predicted ratings are to the actual ratings, respectively.\n",
    "- **Results**: The K-fold cross-validation results indicate that the neural network model exhibits consistent performance across different subsets of the data, with accuracies ranging from approximately 86.9% to 91.4% across ten folds and an overall average accuracy of 89.7%. The mean squared error (MSE) values, averaging at 0.0516, suggest the model's predictions are reasonably close to the actual values. This demonstrates the model's robustness and generalizability, confirming its ability to perform well across diverse data segments without significant overfitting or underfitting to the training data.\n",
    "\n",
    "2. **Hyperparameter Tuning**\n",
    "- Hyperparameter Search: Employed a hyperparameter tuning process (using Keras Tuner) to find the optimal model architecture and learning rate, which are crucial for achieving the best possible model performance.\n",
    "- GridSearch: A GridSearch approach was selected, systematically exploring a range of predefined hyperparameter values to find the best combination, focusing on maximizing validation accuracy.\n",
    "- Optimization Results: The process identified the optimal number of units in each layer and the learning rate for the SGD optimizer, which were then used to build and evaluate the best model configuration.\n",
    "- **Results**: The hyperparameter tuning results reveal the identification of an optimal model configuration with 312 units in the first hidden layer and a learning rate of approximately 0.000202. This configuration resulted in a significant improvement in the model's performance during the initial training phases, showcasing high validation accuracies that indicate the model's capacity to make accurate predictions. The process of hyperparameter tuning has effectively pinpointed the most conducive parameters for maximizing the model’s accuracy, underlining the critical role of tuning in enhancing the predictive power and efficiency of machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of the insights derived from the performance of our current neural network model, we are considering the exploration of two additional machine learning models to potentially enhance our predictive capabilities and address the identified limitations:\n",
    "\n",
    "**Decision Tree Classifier**: Despite the simplicity of decision trees, they are powerful for classification tasks and provide clear visualization of the decision-making process. A key advantage of using a Decision Tree Classifier is its interpretability; it allows for easy understanding of how decisions are made, which is invaluable for analyzing which features most significantly affect a movie's success. Furthermore, decision trees can handle non-linear data effectively and are less susceptible to outliers than regression models, making them a suitable choice for further exploration.\n",
    "\n",
    "These models were chosen with the intention of addressing specific challenges observed in our initial approach. Polynomial regression will allow us to test the hypothesis that a more nuanced modeling of relationships between variables could yield better predictive performance. On the other hand, the Decision Tree Classifier offers a different approach to classification, with the potential for high "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the conclusion of your 2nd model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model demonstrates good performance in terms of accuracy, precision, and recall on both the training and validation datasets. These metrics are indicative of a model that is correctly identifying the majority of instances across the classes it was trained to predict. The consistent high performance on unseen validation data suggests that the model has generalized well beyond the training dataset.\n",
    "\n",
    "However, it is important to consider the confusion matrix, which reveals that while the model excels at predicting a certain class, it may not be performing equally well across all classes. This could be a sign of class imbalance or that the model's predictive features such as 'Gross Revenue' and 'Budget' are particularly informative for one class but less so for others.\n",
    "\n",
    "In conclusion, the second model is a robust classifier for the 'group ratings' based on the 'Gross Revenue' and 'Budget' features. It presents a high degree of accuracy, precision, and recall, which are strong indicators of its reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can be done to possibly to improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refine our approach and enhance the model's predictive accuracy and reliability, we propose several strategies:\n",
    "\n",
    "- **Data Augmentation**: Expanding our dataset with more variables or by integrating additional datasets could provide a richer context for analysis, helping the model to uncover more nuanced relationships between features and outcomes.\n",
    "\n",
    "- **Advanced Feature Engineering**: Delving deeper into feature selection and transformation to emphasize more predictive variables. This could involve more sophisticated techniques to extract or combine features in ways that better capture the complexities of movie success.\n",
    "\n",
    "- **Model Complexity Adjustment**: Experimenting with the neural network's architecture, such as layer depth and neuron count, to strike an optimal balance between model complexity and overfitting. This includes evaluating different activation functions, optimizers, and regularization methods.\n",
    "\n",
    "- **Ensemble Methods**: Considering ensemble techniques, such as bagging or boosting, to improve model stability and accuracy. These methods can aggregate predictions from multiple models to reduce variance and bias.\n",
    "\n",
    "- **Exploration of Alternative Models**: As previously mentioned, we plan to explore Polynomial Regression and Decision Tree Classifier models. These alternatives could offer new perspectives and methodologies for addressing the dataset's challenges, potentially overcoming limitations observed in the neural network model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
